# -*- coding: utf-8 -*-
"""Homework01_Abby_Stucki.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MU_8QzrbwSjDyj7v2v9sC4cJhgH8ajOG
"""

# Question 1: Discretization
# Given a dataset of temperatures (in Fahrenheit) recorded over a week,
# Discretize temperatures into assigned categories

import pandas as pd
import math

temps = [72, 85, 90, 77, 65, 80, 95, 102, 60, 68, 88, 73, 78, 69, 91, -10, -5, -20]

# Cold, -inf to 69
# Warm, 70 to 85
# Hot, 86 to 99
# Very Hot, 100 to inf
bins = [-float(math.inf), 69, 85, 99, float(math.inf)]
labels = ["Cold", "Warm", "Hot", "Very Hot"]

discretized_temps = pd.cut(temps, bins=bins, labels=labels)

print(discretized_temps)

# The output will demonstrate sorting numerical values into discrete categories

# Question 2: Numeric Coding of Nominal/Ordinal Attributes
# 2A: One-Hot Encoding Using OneHotEncoder
# Convert list into numeric codes

from sklearn.preprocessing import OneHotEncoder

# place data into dataframe
car_brands = ["Toyota", "Ford", "Honda", "Toyota", "BMW", "Ford", "Honda"]
car_df = pd.DataFrame(car_brands)

# encode dataframe
encoder = OneHotEncoder(sparse_output=False)
one_hot_df = pd.DataFrame(encoder.fit_transform(car_df))

print(one_hot_df)

# The output encodes the brands accordingly into unique bins

# 2B: Ordinal Encding Using OrdinalEncoder
# Convert brand and size into numeric values

from sklearn.preprocessing import OrdinalEncoder

# place data into dataframe
car_info = {
    'Brand': ["Toyota", "Ford", "Honda", "Toyota", "BMW", "Ford", "Honda"],
    'Size': ["M", "L", "S", "XL", "M", "S", "L"]
}
car_df = pd.DataFrame(car_info)
categories = car_df.select_dtypes(include=['object']).columns.tolist()

# encode dataframe
encoder = OrdinalEncoder()
ordinal_encoded = encoder.fit_transform(car_df[categories])
ordinal_df = pd.DataFrame(ordinal_encoded, columns=encoder.get_feature_names_out(categories))

print(ordinal_df)

# The output demonstrates encoding more than one category into numeric values
# OrdinalEncoder assigns numeric values to each category so that matching values have the same code

# 2C: Numeric Coding Using pandas' Factorize Function
# Encode car brands into numeric codes via pd.factorize()
# Print both the numeric codes and the uniquely identified categories

# factorize data
categories, codes = pd.factorize(["Toyota", "Ford", "Honda", "Toyota", "BMW", "Ford", "Honda"])

print("Numeric Codes: ", categories)
print("Unique Categories: ", codes)

# The output displays which categories were obtained
# The categorical index is used for the numerical code representation of the data
# Factorize returns a 1D array, where the numerical code is determined by order of reading
# Other encoding methods use a different approach to assign codes

# 2D: One-Hot Encding Using pandas' get_dummies Function
# One-hot encode the data using pd.get_dummies()

# place data into dataframe
car_brands = ["Toyota", "Ford", "Honda", "Toyota", "BMW", "Ford", "Honda"]
car_df = pd.DataFrame(car_brands)

print(pd.get_dummies(car_df))

# The output shows the data sorted into the table
# One-hot ordinal encoding "prioritizes" data codes by alphabetical order
# This can be helpful in certain circumstances however a hindrance given large sets of data with many categories

# Question 3: Data Preprocessing and Cleansing
# Take the given dataset with missing values, errors, etc.
# Clean and preprocess the dataset by addressing issues

import numpy as np

# create and print dataset with all issues included
dataset = {
    'CustomerID' : [101, 102, 103, 104, 105, 106],
    'Age' : [25, np.nan, 31, -22, 28, 35],
    'Income' : [50000, 62000, np.nan, 45000, 78000, 88000],
    'Gender' : ["Male", "Female", "Male", "Unknown", "Female", "F"],
    'JoinedDate' : ["2022-01-15", "2022/01/20", "15-01-2022", "2022-01-22", np.nan, "2022-01-25"]
}
messy_df = pd.DataFrame(dataset)
print(messy_df)

# assign date format to single matching format, replace NaT
messy_df['JoinedDate'] = pd.to_datetime(messy_df.JoinedDate, format='mixed')
messy_df.replace({pd.to_datetime('NaT'): messy_df['JoinedDate'].median()}, inplace=True)

# replace numerical NaN values with average/mean of categories
messy_df['Age'] = messy_df['Age'].abs()
messy_df['Age'].fillna(messy_df['Age'].mean(), inplace=True)
messy_df['Income'].fillna(messy_df['Income'].mean(), inplace=True)
messy_df.replace({'F': 'Female'}, inplace=True)
messy_df['Gender'].replace({'Unknown': 'Female'}, inplace=True)

print(messy_df)

# The resulting dataset has replaced unknown/NaN values with the mean or median of its category

# Question 4: Feature Selection
# 4A: Recursive Feature Elimination
# Use RFE with linear regression to select the top 3 features to predict median house value

from sklearn.datasets import fetch_california_housing
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

housing = fetch_california_housing()
x = housing.data
y = housing.target

estimate = LinearRegression()
select = RFE(estimate, n_features_to_select=3, step=15)
select = select.fit(x, y)
x_select = select.transform(x)
features = select.support_
ranking = select.ranking_
feature_names = [housing.feature_names[i] for i, bool_val in enumerate(features) if bool_val]

print("Selected features:", feature_names)

# RFE removes least important features to optimize model performance until n features remain
# This is helpful for dimensionality reduction with large quantities of data
# Median income obviously effects the cost of a house
# The more bedrooms in a house, the more influence on the house
# Longitude determines where in California the house is, most likely in reference to near-ness to the coast

# 4B: Feature Importance From Tree-Based Models
# Use Random Forest to determine top 3 features for prediction

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFECV

housing = fetch_california_housing()
x = housing.data
y = housing.target

estimate = RandomForestRegressor(random_state=0)
select = RFE(estimate, n_features_to_select=3, step=15)
select = select.fit(x, y)
x_select = select.transform(x)
features = select.support_
ranking = select.ranking_
feature_names = [housing.feature_names[i] for i, bool_val in enumerate(features) if bool_val]

print("Selected features:", feature_names)

# Median income effects the cost of a house
# The average occupation feeds into the cost/affordability of a house
# Latitude will also contribute to housing location

# 4C: L1-Based Feature Selection (Lasso Regularization)
# Use Lasso Regularization to perform feature selection by shrinking coefficients

from sklearn.linear_model import Lasso

housing = fetch_california_housing()
x = housing.data
y = housing.target

estimate = Lasso()
select = RFE(estimate, n_features_to_select=3, step=15)
select = select.fit(x, y)
x_select = select.transform(x)
features = select.support_
ranking = select.ranking_
feature_names = [housing.feature_names[i] for i, bool_val in enumerate(features) if bool_val]

print("Selected features:", feature_names)

# Median income effects the cost of a house
# The age of the house determines its risk and thus influences cost
# The population of an area contributes to housing prices

# 4D: Mutual information Feature Selection
# Use Mutual Information Feature Selection to identify the top 3 features
# Determines highest dependency on target variable

from sklearn.feature_selection import SelectKBest, mutual_info_regression

housing = fetch_california_housing()
x = housing.data
y = housing.target

select = SelectKBest(score_func=mutual_info_regression, k=3)
select = select.fit(x,y)
x_select = select.transform(x)
features = select.get_support()
feature_names = [housing.feature_names[i] for i, bool_val in enumerate(features) if bool_val]

print("Selected features:", feature_names)

# This method evaluates features off of the mutual information scored with the target
# Dependency of variables is key to this approach

# 4E: Sequential Feature Selection
# Use Sequential Feature Selection to identify the top 3 features

from sklearn.feature_selection import SequentialFeatureSelector

housing = fetch_california_housing()
x = housing.data
y = housing.target

estimate = LinearRegression()
select = SequentialFeatureSelector(estimate, n_features_to_select=3)
select = select.fit(x, y)
x_select = select.transform(x)
features = select.support_
feature_names = [housing.feature_names[i] for i, bool_val in enumerate(features) if bool_val]

print("Selected features:", feature_names)

# This approach can either work with forward or backward selection on features
# It is very helpful when focusing on the performance of the model

# Question 5: Data Transformation
# Apply a square root transformation to reduce skewness of dataset

import matplotlib.pyplot as plt

dataset = {
    'HouseID': [1, 2, 3, 4, 5, 6, 7],
    'LotSize': [400, 1600, 2500, 3600, 4900, 6400, 8100]
}

df = pd.DataFrame(dataset)
df2 = pd.DataFrame(dataset)
df2['LotSize'] = np.sqrt(df2['LotSize'])

fig, axs = plt.subplots(nrows=1, ncols=2)

axs[0].plot(df['HouseID'], df['LotSize'])
axs[0].set_title('Original')
axs[1].plot(df2['HouseID'], df2['LotSize'])
axs[1].set_title('Transformed')

# The square root was applied to normalize the data and make it more manageable
# The transformation inverted the curve which helps give a more accurate analysis of the relationship and dependencies